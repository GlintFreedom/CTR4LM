Tokenizer loaded

	Tokenizer type: BertTokenizerFast
	PL's path: models/bert-base-uncased
	Size of vocab: 30522
	Max length of input: 512
	Special tokens: unk_token: [UNK], sep_token: [SEP], pad_token: [PAD], cls_token: [CLS], mask_token: [MASK]
Tokenizer extended

	Tokenizer type: BertTokenizerFast
	PL's path: models/bert-base-uncased
	Size of vocab: 40055
	Max length of input: 512
	Special tokens: unk_token: [UNK], sep_token: [SEP], pad_token: [PAD], cls_token: [CLS], mask_token: [MASK]
MyDataset object with 799448 samples
MyDataset object with 99900 samples
MyDataset object with 99900 samples
Using device: cuda
